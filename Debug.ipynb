{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074171a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d94bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from lstmppo.env import make_env, RecurrentVecEnvWrapper\n",
    "from lstmppo.buffer import RecurrentRolloutBuffer\n",
    "from lstmppo.policy import LSTMPPOPolicy\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "from lstmppo.init import initialize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from lstmppo.types import RolloutStep\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724cc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"\"]\n",
    "cfg = initialize(seconds_since_epoch=1766683383)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.weight cpu\n",
      "encoder.0.bias cpu\n",
      "encoder.2.weight cpu\n",
      "encoder.2.bias cpu\n",
      "lstm.weight_hh_l0_raw cpu\n",
      "lstm.module.weight_ih_l0 cpu\n",
      "lstm.module.bias_ih_l0 cpu\n",
      "lstm.module.bias_hh_l0 cpu\n",
      "ln.weight cpu\n",
      "ln.bias cpu\n",
      "actor.weight cpu\n",
      "actor.bias cpu\n",
      "critic.weight cpu\n",
      "critic.bias cpu\n"
     ]
    }
   ],
   "source": [
    "class PPOTrainer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg):\n",
    "        \n",
    "        self.vec_state_env = None\n",
    "        self.global_step = 0\n",
    "        self.rollout_steps = cfg.rollout_steps\n",
    "\n",
    "        self.policy = LSTMPPOPolicy(cfg).to(cfg.device)\n",
    "\n",
    "        venv = SyncVectorEnv([make_env(cfg.env_id)\n",
    "                              for _ in range(cfg.num_envs)])\n",
    "        \n",
    "        self.env = RecurrentVecEnvWrapper(cfg,\n",
    "                                          venv)\n",
    "\n",
    "        self.buffer = RecurrentRolloutBuffer(cfg)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(),\n",
    "                                    lr=cfg.learning_rate,\n",
    "                                    eps=1e-5)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.env_state = self.env.reset()\n",
    "        self.buffer.reset()\n",
    "        self.global_step = 0\n",
    "\n",
    "    def rollout_phase(self):\n",
    "        \n",
    "        for _ in range(self.rollout_steps):\n",
    "\n",
    "            # --- Policy forward ---\n",
    "            policy_in = self.env_state.to_policy_input()\n",
    "            actions, logprobs, policy_out = self.policy.act(policy_in)\n",
    "\n",
    "            # --- Store rollout step ---\n",
    "            self.buffer.add(RolloutStep(\n",
    "                obs=self.env_state.obs,\n",
    "                actions=actions,\n",
    "                rewards=self.env_state.rewards,\n",
    "                values=policy_out.values,\n",
    "                logprobs=logprobs,\n",
    "                terminated=self.env_state.terminated,\n",
    "                truncated=self.env_state.truncated,\n",
    "                hxs=policy_out.new_hxs,\n",
    "                cxs=policy_out.new_cxs,\n",
    "            ))\n",
    "\n",
    "            # --- Step environment ---\n",
    "            self.env_state = self.env.step(actions)\n",
    "\n",
    "            # --- Update hidden states in wrapper ---\n",
    "            self.env.update_hidden_states(policy_out.new_hxs,\n",
    "                                          policy_out.new_cxs)\n",
    "\n",
    "trainer = PPOTrainer(cfg)\n",
    "for name, p in trainer.policy.named_parameters():\n",
    "    print(name, p.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92890cb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m, in \u001b[0;36mPPOTrainer.rollout_phase\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_steps):\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# --- Policy forward ---\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     policy_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_state\u001b[38;5;241m.\u001b[39mto_policy_input()\n\u001b[0;32m---> 40\u001b[0m     actions, logprobs, policy_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# --- Store rollout step ---\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39madd(RolloutStep(\n\u001b[1;32m     44\u001b[0m         obs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_state\u001b[38;5;241m.\u001b[39mobs,\n\u001b[1;32m     45\u001b[0m         actions\u001b[38;5;241m=\u001b[39mactions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m         cxs\u001b[38;5;241m=\u001b[39mpolicy_out\u001b[38;5;241m.\u001b[39mnew_cxs,\n\u001b[1;32m     53\u001b[0m     ))\n",
      "File \u001b[0;32m~/Documents/po_cartpole/lstmppo/policy.py:199\u001b[0m, in \u001b[0;36mLSTMPPOPolicy.act\u001b[0;34m(self, policy_input)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    197\u001b[0m         policy_input: VecPolicyInput):\n\u001b[0;32m--> 199\u001b[0m     policy_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mpolicy_output\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[1;32m    202\u001b[0m     actions \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/Documents/po_cartpole/lstmppo/policy.py:174\u001b[0m, in \u001b[0;36mLSTMPPOPolicy.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp: VecPolicyInput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecPolicyOutput:\n\u001b[1;32m    173\u001b[0m     core_out, new_hxs, new_cxs, ar_loss, tar_loss \u001b[38;5;241m=\u001b[39m\\\n\u001b[0;32m--> 174\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m core_out\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    179\u001b[0m         B, T, H \u001b[38;5;241m=\u001b[39m core_out\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Documents/po_cartpole/lstmppo/policy.py:145\u001b[0m, in \u001b[0;36mLSTMPPOPolicy._forward_core\u001b[0;34m(self, x, hxs, cxs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(enc,\n\u001b[1;32m    147\u001b[0m                             (hxs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    148\u001b[0m                              cxs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    149\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(out)\n",
      "File \u001b[0;32m~/miniconda/envs/cage2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/cage2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/cage2_env/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/cage2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/cage2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/cage2_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "trainer.rollout_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "097e99fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Linear(4, 16)\n",
    "nn.init.xavier_uniform_(a.weight)\n",
    "nn.init.zeros_(a.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b497fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(\n",
    "    env,\n",
    "    policy,\n",
    "    buffer,\n",
    "    optimizer,\n",
    "    num_updates,\n",
    "    rollout_length,\n",
    "    gamma,\n",
    "    lam,\n",
    "    clip_ratio,\n",
    "    entropy_coef,\n",
    "    value_coef,\n",
    "    device,\n",
    "):\n",
    "    # ---------------------------------------------------------\n",
    "    # Reset environment and get initial VecEnvState\n",
    "    # ---------------------------------------------------------\n",
    "    state = env.reset()   # returns VecEnvState\n",
    "\n",
    "    for update in range(num_updates):\n",
    "\n",
    "        buffer.reset()\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Rollout phase\n",
    "        # -----------------------------------------------------\n",
    "        for t in range(rollout_length):\n",
    "\n",
    "            # --- Policy forward ---\n",
    "            policy_in = state.to_policy_input()\n",
    "            policy_out = policy(policy_in)\n",
    "\n",
    "            dist = torch.distributions.Categorical(logits=policy_out.logits)\n",
    "            actions = dist.sample()\n",
    "            logprobs = dist.log_prob(actions)\n",
    "\n",
    "            # --- Store rollout step ---\n",
    "            buffer.add(RolloutStep(\n",
    "                obs=state.obs,\n",
    "                actions=actions,\n",
    "                rewards=state.rewards,\n",
    "                values=policy_out.values,\n",
    "                logprobs=logprobs,\n",
    "                terminated=state.terminated,\n",
    "                truncated=state.truncated,\n",
    "                hxs=policy_out.new_hxs,\n",
    "                cxs=policy_out.new_cxs,\n",
    "            ))\n",
    "\n",
    "            # --- Step environment ---\n",
    "            state = env.step(actions)\n",
    "\n",
    "            # --- Update hidden states in wrapper ---\n",
    "            env.update_hidden_states(policy_out.new_hxs, policy_out.new_cxs)\n",
    "\n",
    "\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # PPO update phase\n",
    "        # -----------------------------------------------------\n",
    "        for batch in buffer.recurrent_minibatches():\n",
    "\n",
    "            # --- Policy forward on batch ---\n",
    "            policy_in = VecPolicyInput(\n",
    "                obs=batch.obs,\n",
    "                hxs=batch.hxs,\n",
    "                cxs=batch.cxs,\n",
    "            )\n",
    "            out = policy(policy_in)\n",
    "\n",
    "            dist = torch.distributions.Categorical(logits=out.logits)\n",
    "            new_logprobs = dist.log_prob(batch.actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # --- PPO losses ---\n",
    "            ratio = (new_logprobs - batch.logprobs).exp()\n",
    "            surr1 = ratio * batch.advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * batch.advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = ((out.values - batch.returns) ** 2).mean()\n",
    "\n",
    "            # --- Add AR/TAR ---\n",
    "            ar_tar_loss = out.ar_loss + out.tar_loss\n",
    "\n",
    "            # --- Total loss ---\n",
    "            loss = (\n",
    "                policy_loss\n",
    "                + value_coef * value_loss\n",
    "                - entropy_coef * entropy\n",
    "                + ar_tar_loss\n",
    "            )\n",
    "\n",
    "            # --- Optimize ---\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Logging (optional)\n",
    "        # -----------------------------------------------------\n",
    "        print(f\"Update {update}: loss={loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4608714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_ppo_position_only_cartpole(cfg: PPOConfig):\n",
    "    device = torch.device(cfg.device)\n",
    "    env_id = \"POPGym-PositionOnlyCartPole-v0\"  # check id if needed\n",
    "\n",
    "    # Vectorized env\n",
    "    venv = SyncVectorEnv([make_env(env_id) for _ in range(cfg.num_envs)])\n",
    "    dummy_env = gym.make(env_id)\n",
    "    obs_shape = dummy_env.observation_space.shape\n",
    "    action_dim = dummy_env.action_space.n\n",
    "    dummy_env.close()\n",
    "\n",
    "    # Policy and optimizer\n",
    "    policy = LSTMPPOPolicy(\n",
    "        obs_dim=obs_shape[0],\n",
    "        action_dim=action_dim,\n",
    "        hidden_size=cfg.hidden_size\n",
    "    ).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "\n",
    "        # ==== PPO UPDATE ====\n",
    "        for epoch in range(cfg.update_epochs):\n",
    "            for mb in buffer.get_recurrent_minibatches(cfg.batch_envs):\n",
    "                mb_obs = mb[\"obs\"]          # (T,B,obs)\n",
    "                mb_actions = mb[\"actions\"]  # (T,B,1)\n",
    "                mb_returns = mb[\"returns\"]  # (T,B)\n",
    "                mb_advantages = mb[\"advantages\"]  # (T,B)\n",
    "                mb_logprobs_old = mb[\"logprobs\"]  # (T,B)\n",
    "                mb_hxs = mb[\"hxs\"]          # (B,H)\n",
    "                mb_cxs = mb[\"cxs\"]          # (B,H)\n",
    "\n",
    "                # Normalize advantages per minibatch\n",
    "                adv = mb_advantages\n",
    "                adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "                # Evaluate actions\n",
    "                new_logprobs, entropy, values = policy.evaluate_actions(\n",
    "                    mb_obs, mb_hxs, mb_cxs, mb_actions\n",
    "                )\n",
    "\n",
    "                # PPO ratio\n",
    "                ratio = (new_logprobs - mb_logprobs_old).exp()\n",
    "\n",
    "                # Policy loss\n",
    "                unclipped = ratio * adv\n",
    "                clipped = torch.clamp(ratio, 1.0 - cfg.clip_coef, 1.0 + cfg.clip_coef) * adv\n",
    "                policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values, mb_returns)\n",
    "\n",
    "                # Entropy bonus\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = policy_loss + cfg.vf_coef * value_loss - cfg.ent_coef * entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(policy.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "        # Logging (replace with your logger)\n",
    "        avg_return = mb_returns.mean().item()\n",
    "        print(f\"Step: {global_step}, ApproxReturn(last mb): {avg_return:.2f}\")\n",
    "\n",
    "    env.venv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07dd3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "import popgym\n",
    "from gymnasium.vector import SyncVectorEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b988d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class RecurrentVecEnvWrapper:\n",
    "    \"\"\"\n",
    "    Wraps a vectorized Gymnasium environment and manages\n",
    "    per-environment LSTM hidden states.\n",
    "\n",
    "    Works with SyncVectorEnv or AsyncVectorEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, venv, hidden_size, device):\n",
    "        self.venv = venv\n",
    "        self.num_envs = venv.num_envs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        # Hidden states per environment\n",
    "        self.hxs = torch.zeros(self.num_envs, hidden_size, device=device)\n",
    "        self.cxs = torch.zeros(self.num_envs, hidden_size, device=device)\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.venv.reset()\n",
    "\n",
    "        # Reset all hidden states\n",
    "        self.hxs.zero_()\n",
    "        self.cxs.zero_()\n",
    "\n",
    "        return torch.tensor(obs, device=self.device), info, self.hxs.clone(), self.cxs.clone()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        actions: (N, action_dim)\n",
    "        Returns:\n",
    "            obs: (N, obs_dim)\n",
    "            rewards: (N,)\n",
    "            terminated: (N,)\n",
    "            truncated: (N,)\n",
    "            info: list of dicts\n",
    "            hxs: (N, hidden_size)\n",
    "            cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        obs, rewards, terminated, truncated, info = self.venv.step(actions.cpu().numpy())\n",
    "\n",
    "        terminated = torch.tensor(terminated, device=self.device, dtype=torch.bool)\n",
    "        truncated = torch.tensor(truncated, device=self.device, dtype=torch.bool)\n",
    "\n",
    "        # Reset hidden states only for true terminals\n",
    "        done_mask = terminated  # NOT truncated\n",
    "        if done_mask.any():\n",
    "            self.hxs[done_mask] = 0\n",
    "            self.cxs[done_mask] = 0\n",
    "\n",
    "        return (\n",
    "            torch.tensor(obs, device=self.device),\n",
    "            torch.tensor(rewards, device=self.device),\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "            self.hxs.clone(),\n",
    "            self.cxs.clone(),\n",
    "        )\n",
    "\n",
    "    def update_hidden_states(self, new_hxs, new_cxs):\n",
    "        \"\"\"\n",
    "        Called after the policy forward pass.\n",
    "        new_hxs, new_cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        self.hxs.copy_(new_hxs)\n",
    "        self.cxs.copy_(new_cxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RecurrentRolloutBuffer:\n",
    "    def __init__(self, T, N, obs_shape, action_dim, hidden_size, device):\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.device = device\n",
    "\n",
    "        # Core storage\n",
    "        self.obs = torch.zeros(T, N, *obs_shape, device=device)\n",
    "        self.actions = torch.zeros(T, N, action_dim, device=device)\n",
    "        self.rewards = torch.zeros(T, N, device=device)\n",
    "        self.values = torch.zeros(T, N, device=device)\n",
    "        self.logprobs = torch.zeros(T, N, device=device)\n",
    "\n",
    "        # Episode termination logic\n",
    "        self.terminated = torch.zeros(T, N, device=device, dtype=torch.bool)\n",
    "        self.truncated = torch.zeros(T, N, device=device, dtype=torch.bool)\n",
    "\n",
    "        # Hidden states at the *start* of each timestep\n",
    "        # Shape: (T, N, num_layers, hidden_size)\n",
    "        self.hxs = torch.zeros(T, N, hidden_size, device=device)\n",
    "        self.cxs = torch.zeros(T, N, hidden_size, device=device)\n",
    "\n",
    "        # Filled index\n",
    "        self.step = 0\n",
    "\n",
    "    def add(self, obs, actions, rewards, values, logprobs,\n",
    "            terminated, truncated, hxs, cxs):\n",
    "        \"\"\"\n",
    "        obs: (N, obs_dim)\n",
    "        hxs, cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        t = self.step\n",
    "        self.obs[t].copy_(obs)\n",
    "        self.actions[t].copy_(actions)\n",
    "        self.rewards[t].copy_(rewards)\n",
    "        self.values[t].copy_(values)\n",
    "        self.logprobs[t].copy_(logprobs)\n",
    "        self.terminated[t].copy_(terminated)\n",
    "        self.truncated[t].copy_(truncated)\n",
    "        self.hxs[t].copy_(hxs)\n",
    "        self.cxs[t].copy_(cxs)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "    def compute_gae(self, last_value, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        last_value: (N,)\n",
    "        \"\"\"\n",
    "        T, N = self.T, self.N\n",
    "\n",
    "        advantages = torch.zeros(T, N, device=self.device)\n",
    "        last_gae = torch.zeros(N, device=self.device)\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            # True terminal: no bootstrap\n",
    "            true_terminal = self.terminated[t]\n",
    "\n",
    "            # Time-limit truncation: DO bootstrap\n",
    "            bootstrap = ~true_terminal\n",
    "\n",
    "            next_value = last_value if t == T - 1 else self.values[t + 1]\n",
    "\n",
    "            delta = (\n",
    "                self.rewards[t]\n",
    "                + gamma * next_value * bootstrap\n",
    "                - self.values[t]\n",
    "            )\n",
    "\n",
    "            last_gae = delta + gamma * lam * last_gae * bootstrap\n",
    "            advantages[t] = last_gae\n",
    "\n",
    "        returns = advantages + self.values\n",
    "        self.advantages = advantages\n",
    "        self.returns = returns\n",
    "\n",
    "    def get_recurrent_minibatches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns sequences of shape:\n",
    "        (seq_len=T, batch_size, ...)\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        env_indices = torch.randperm(N)\n",
    "\n",
    "        for start in range(0, N, batch_size):\n",
    "            idx = env_indices[start:start + batch_size]\n",
    "\n",
    "            yield {\n",
    "                \"obs\": self.obs[:, idx],\n",
    "                \"actions\": self.actions[:, idx],\n",
    "                \"values\": self.values[:, idx],\n",
    "                \"logprobs\": self.logprobs[:, idx],\n",
    "                \"returns\": self.returns[:, idx],\n",
    "                \"advantages\": self.advantages[:, idx],\n",
    "                \"hxs\": self.hxs[0, idx],  # initial hidden state\n",
    "                \"cxs\": self.cxs[0, idx],\n",
    "                \"terminated\": self.terminated[:, idx],\n",
    "                \"truncated\": self.truncated[:, idx],\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cage2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
