{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074171a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d94bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from lstmppo.env import make_env, RecurrentVecEnvWrapper\n",
    "from lstmppo.buffer import RecurrentRolloutBuffer\n",
    "from lstmppo.policy import LSTMPPOPolicy\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "from lstmppo.init import initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724cc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"\"]\n",
    "cfg = initialize(seconds_since_epoch=1766683383)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f039dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPOTrainer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg):\n",
    "        \n",
    "        self.vec_state_env = None\n",
    "        self.global_step = 0\n",
    "        self.rollout_steps = cfg.rollout_steps\n",
    "\n",
    "        self.policy = LSTMPPOPolicy(cfg).to(cfg.device)\n",
    "\n",
    "        venv = SyncVectorEnv([make_env(cfg.env_id)\n",
    "                              for _ in range(cfg.num_envs)])\n",
    "        \n",
    "        self.env = RecurrentVecEnvWrapper(cfg,\n",
    "                                          venv)\n",
    "\n",
    "        self.buffer = RecurrentRolloutBuffer(cfg)\n",
    "\n",
    "        self.policy = LSTMPPOPolicy(cfg)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(),\n",
    "                                    lr=cfg.learning_rate,\n",
    "                                    eps=1e-5)\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.vec_state_env = self.env.reset()\n",
    "        self.buffer.reset()\n",
    "        self.global_step = 0\n",
    "\n",
    "    def collect_rollout(self):\n",
    "\n",
    "        for _ in range(cfg.rollout_steps):\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                policy =\\\n",
    "                    self.policy.act(self.vec_state_env.to_policy_input())\n",
    "\n",
    "            next_obs, rewards, terminated, truncated, info, hxs_env, cxs_env = env.step(actions)\n",
    "\n",
    "            # Store step (use hxs_env/cxs_env = hidden state at start of step)\n",
    "            buffer.add(\n",
    "                obs=obs,\n",
    "                actions=actions,\n",
    "                rewards=rewards,\n",
    "                values=values,\n",
    "                logprobs=logprobs,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "                hxs=hxs_env,\n",
    "                cxs=cxs_env,\n",
    "            )\n",
    "\n",
    "            # Update hidden states inside env wrapper to new policy states\n",
    "            env.update_hidden_states(new_hxs, new_cxs)\n",
    "\n",
    "            obs = next_obs\n",
    "            hxs = new_hxs\n",
    "            cxs = new_cxs\n",
    "\n",
    "            global_step += cfg.num_envs\n",
    "\n",
    "        # Bootstrap value at last obs\n",
    "        with torch.no_grad():\n",
    "            last_logits, last_values, _, _ = policy.forward(obs, hxs, cxs)\n",
    "            last_value = last_values  # (N,)\n",
    "\n",
    "        buffer.compute_gae(last_value=last_value, gamma=cfg.gamma, lam=cfg.gae_lambda)\n",
    "\n",
    "trainer = PPOTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b497fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4608714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_ppo_position_only_cartpole(cfg: PPOConfig):\n",
    "    device = torch.device(cfg.device)\n",
    "    env_id = \"POPGym-PositionOnlyCartPole-v0\"  # check id if needed\n",
    "\n",
    "    # Vectorized env\n",
    "    venv = SyncVectorEnv([make_env(env_id) for _ in range(cfg.num_envs)])\n",
    "    dummy_env = gym.make(env_id)\n",
    "    obs_shape = dummy_env.observation_space.shape\n",
    "    action_dim = dummy_env.action_space.n\n",
    "    dummy_env.close()\n",
    "\n",
    "    # Policy and optimizer\n",
    "    policy = LSTMPPOPolicy(\n",
    "        obs_dim=obs_shape[0],\n",
    "        action_dim=action_dim,\n",
    "        hidden_size=cfg.hidden_size\n",
    "    ).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "\n",
    "        # ==== PPO UPDATE ====\n",
    "        for epoch in range(cfg.update_epochs):\n",
    "            for mb in buffer.get_recurrent_minibatches(cfg.batch_envs):\n",
    "                mb_obs = mb[\"obs\"]          # (T,B,obs)\n",
    "                mb_actions = mb[\"actions\"]  # (T,B,1)\n",
    "                mb_returns = mb[\"returns\"]  # (T,B)\n",
    "                mb_advantages = mb[\"advantages\"]  # (T,B)\n",
    "                mb_logprobs_old = mb[\"logprobs\"]  # (T,B)\n",
    "                mb_hxs = mb[\"hxs\"]          # (B,H)\n",
    "                mb_cxs = mb[\"cxs\"]          # (B,H)\n",
    "\n",
    "                # Normalize advantages per minibatch\n",
    "                adv = mb_advantages\n",
    "                adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "                # Evaluate actions\n",
    "                new_logprobs, entropy, values = policy.evaluate_actions(\n",
    "                    mb_obs, mb_hxs, mb_cxs, mb_actions\n",
    "                )\n",
    "\n",
    "                # PPO ratio\n",
    "                ratio = (new_logprobs - mb_logprobs_old).exp()\n",
    "\n",
    "                # Policy loss\n",
    "                unclipped = ratio * adv\n",
    "                clipped = torch.clamp(ratio, 1.0 - cfg.clip_coef, 1.0 + cfg.clip_coef) * adv\n",
    "                policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values, mb_returns)\n",
    "\n",
    "                # Entropy bonus\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = policy_loss + cfg.vf_coef * value_loss - cfg.ent_coef * entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(policy.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "        # Logging (replace with your logger)\n",
    "        avg_return = mb_returns.mean().item()\n",
    "        print(f\"Step: {global_step}, ApproxReturn(last mb): {avg_return:.2f}\")\n",
    "\n",
    "    env.venv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07dd3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "import popgym\n",
    "from gymnasium.vector import SyncVectorEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b988d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class RecurrentVecEnvWrapper:\n",
    "    \"\"\"\n",
    "    Wraps a vectorized Gymnasium environment and manages\n",
    "    per-environment LSTM hidden states.\n",
    "\n",
    "    Works with SyncVectorEnv or AsyncVectorEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, venv, hidden_size, device):\n",
    "        self.venv = venv\n",
    "        self.num_envs = venv.num_envs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        # Hidden states per environment\n",
    "        self.hxs = torch.zeros(self.num_envs, hidden_size, device=device)\n",
    "        self.cxs = torch.zeros(self.num_envs, hidden_size, device=device)\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.venv.reset()\n",
    "\n",
    "        # Reset all hidden states\n",
    "        self.hxs.zero_()\n",
    "        self.cxs.zero_()\n",
    "\n",
    "        return torch.tensor(obs, device=self.device), info, self.hxs.clone(), self.cxs.clone()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        actions: (N, action_dim)\n",
    "        Returns:\n",
    "            obs: (N, obs_dim)\n",
    "            rewards: (N,)\n",
    "            terminated: (N,)\n",
    "            truncated: (N,)\n",
    "            info: list of dicts\n",
    "            hxs: (N, hidden_size)\n",
    "            cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        obs, rewards, terminated, truncated, info = self.venv.step(actions.cpu().numpy())\n",
    "\n",
    "        terminated = torch.tensor(terminated, device=self.device, dtype=torch.bool)\n",
    "        truncated = torch.tensor(truncated, device=self.device, dtype=torch.bool)\n",
    "\n",
    "        # Reset hidden states only for true terminals\n",
    "        done_mask = terminated  # NOT truncated\n",
    "        if done_mask.any():\n",
    "            self.hxs[done_mask] = 0\n",
    "            self.cxs[done_mask] = 0\n",
    "\n",
    "        return (\n",
    "            torch.tensor(obs, device=self.device),\n",
    "            torch.tensor(rewards, device=self.device),\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "            self.hxs.clone(),\n",
    "            self.cxs.clone(),\n",
    "        )\n",
    "\n",
    "    def update_hidden_states(self, new_hxs, new_cxs):\n",
    "        \"\"\"\n",
    "        Called after the policy forward pass.\n",
    "        new_hxs, new_cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        self.hxs.copy_(new_hxs)\n",
    "        self.cxs.copy_(new_cxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RecurrentRolloutBuffer:\n",
    "    def __init__(self, T, N, obs_shape, action_dim, hidden_size, device):\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.device = device\n",
    "\n",
    "        # Core storage\n",
    "        self.obs = torch.zeros(T, N, *obs_shape, device=device)\n",
    "        self.actions = torch.zeros(T, N, action_dim, device=device)\n",
    "        self.rewards = torch.zeros(T, N, device=device)\n",
    "        self.values = torch.zeros(T, N, device=device)\n",
    "        self.logprobs = torch.zeros(T, N, device=device)\n",
    "\n",
    "        # Episode termination logic\n",
    "        self.terminated = torch.zeros(T, N, device=device, dtype=torch.bool)\n",
    "        self.truncated = torch.zeros(T, N, device=device, dtype=torch.bool)\n",
    "\n",
    "        # Hidden states at the *start* of each timestep\n",
    "        # Shape: (T, N, num_layers, hidden_size)\n",
    "        self.hxs = torch.zeros(T, N, hidden_size, device=device)\n",
    "        self.cxs = torch.zeros(T, N, hidden_size, device=device)\n",
    "\n",
    "        # Filled index\n",
    "        self.step = 0\n",
    "\n",
    "    def add(self, obs, actions, rewards, values, logprobs,\n",
    "            terminated, truncated, hxs, cxs):\n",
    "        \"\"\"\n",
    "        obs: (N, obs_dim)\n",
    "        hxs, cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        t = self.step\n",
    "        self.obs[t].copy_(obs)\n",
    "        self.actions[t].copy_(actions)\n",
    "        self.rewards[t].copy_(rewards)\n",
    "        self.values[t].copy_(values)\n",
    "        self.logprobs[t].copy_(logprobs)\n",
    "        self.terminated[t].copy_(terminated)\n",
    "        self.truncated[t].copy_(truncated)\n",
    "        self.hxs[t].copy_(hxs)\n",
    "        self.cxs[t].copy_(cxs)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "    def compute_gae(self, last_value, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        last_value: (N,)\n",
    "        \"\"\"\n",
    "        T, N = self.T, self.N\n",
    "\n",
    "        advantages = torch.zeros(T, N, device=self.device)\n",
    "        last_gae = torch.zeros(N, device=self.device)\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            # True terminal: no bootstrap\n",
    "            true_terminal = self.terminated[t]\n",
    "\n",
    "            # Time-limit truncation: DO bootstrap\n",
    "            bootstrap = ~true_terminal\n",
    "\n",
    "            next_value = last_value if t == T - 1 else self.values[t + 1]\n",
    "\n",
    "            delta = (\n",
    "                self.rewards[t]\n",
    "                + gamma * next_value * bootstrap\n",
    "                - self.values[t]\n",
    "            )\n",
    "\n",
    "            last_gae = delta + gamma * lam * last_gae * bootstrap\n",
    "            advantages[t] = last_gae\n",
    "\n",
    "        returns = advantages + self.values\n",
    "        self.advantages = advantages\n",
    "        self.returns = returns\n",
    "\n",
    "    def get_recurrent_minibatches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns sequences of shape:\n",
    "        (seq_len=T, batch_size, ...)\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        env_indices = torch.randperm(N)\n",
    "\n",
    "        for start in range(0, N, batch_size):\n",
    "            idx = env_indices[start:start + batch_size]\n",
    "\n",
    "            yield {\n",
    "                \"obs\": self.obs[:, idx],\n",
    "                \"actions\": self.actions[:, idx],\n",
    "                \"values\": self.values[:, idx],\n",
    "                \"logprobs\": self.logprobs[:, idx],\n",
    "                \"returns\": self.returns[:, idx],\n",
    "                \"advantages\": self.advantages[:, idx],\n",
    "                \"hxs\": self.hxs[0, idx],  # initial hidden state\n",
    "                \"cxs\": self.cxs[0, idx],\n",
    "                \"terminated\": self.terminated[:, idx],\n",
    "                \"truncated\": self.truncated[:, idx],\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cage2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
