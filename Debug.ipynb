{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074171a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d94bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from lstmppo.env import make_env, RecurrentVecEnvWrapper\n",
    "from lstmppo.buffer import RecurrentRolloutBuffer\n",
    "from lstmppo.policy import LSTMPPOPolicy\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "from lstmppo.init import initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724cc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"\"]\n",
    "cfg = initialize(seconds_since_epoch=1766683383)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f039dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from lstmppo.types import RolloutStep\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2612f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01memail\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m policy\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDocuments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpo_cartpole\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlstmppo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPPOTrainer\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      9\u001b[0m                  cfg):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Documents'"
     ]
    }
   ],
   "source": [
    "\n",
    "from email import policy\n",
    "from po_cartpole.lstmppo import env\n",
    "\n",
    "\n",
    "class PPOTrainer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg):\n",
    "        \n",
    "        self.vec_state_env = None\n",
    "        self.global_step = 0\n",
    "        self.rollout_steps = cfg.rollout_steps\n",
    "\n",
    "        self.policy = LSTMPPOPolicy(cfg).to(cfg.device)\n",
    "\n",
    "        venv = SyncVectorEnv([make_env(cfg.env_id)\n",
    "                              for _ in range(cfg.num_envs)])\n",
    "        \n",
    "        self.env = RecurrentVecEnvWrapper(cfg,\n",
    "                                          venv)\n",
    "\n",
    "        self.buffer = RecurrentRolloutBuffer(cfg)\n",
    "\n",
    "        self.policy = LSTMPPOPolicy(cfg)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(),\n",
    "                                    lr=cfg.learning_rate,\n",
    "                                    eps=1e-5)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.env_state = self.env.reset()\n",
    "        self.buffer.reset()\n",
    "        self.global_step = 0\n",
    "\n",
    "    def rollout_phase(self):\n",
    "        \n",
    "        for _ in range(self.rollout_length):\n",
    "\n",
    "            # --- Policy forward ---\n",
    "            policy_in = self.env_state.to_policy_input()\n",
    "            actions, logprobs, policy_out = self.policy.act(policy_in)\n",
    "\n",
    "            # --- Store rollout step ---\n",
    "            self.buffer.add(RolloutStep(\n",
    "                obs=self.env_state.obs,\n",
    "                actions=actions,\n",
    "                rewards=self.env_state.rewards,\n",
    "                values=policy_out.values,\n",
    "                logprobs=logprobs,\n",
    "                terminated=self.env_state.terminated,\n",
    "                truncated=self.env_state.truncated,\n",
    "                hxs=policy_out.new_hxs,\n",
    "                cxs=policy_out.new_cxs,\n",
    "            ))\n",
    "\n",
    "            # --- Step environment ---\n",
    "            self.env_state = self.env.step(actions)\n",
    "\n",
    "            # --- Update hidden states in wrapper ---\n",
    "            self.env.update_hidden_states(policy_out.new_hxs,\n",
    "                                          policy_out.new_cxs)\n",
    "\n",
    "trainer = PPOTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999209d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b497fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(\n",
    "    env,\n",
    "    policy,\n",
    "    buffer,\n",
    "    optimizer,\n",
    "    num_updates,\n",
    "    rollout_length,\n",
    "    gamma,\n",
    "    lam,\n",
    "    clip_ratio,\n",
    "    entropy_coef,\n",
    "    value_coef,\n",
    "    device,\n",
    "):\n",
    "    # ---------------------------------------------------------\n",
    "    # Reset environment and get initial VecEnvState\n",
    "    # ---------------------------------------------------------\n",
    "    state = env.reset()   # returns VecEnvState\n",
    "\n",
    "    for update in range(num_updates):\n",
    "\n",
    "        buffer.reset()\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Rollout phase\n",
    "        # -----------------------------------------------------\n",
    "        for t in range(rollout_length):\n",
    "\n",
    "            # --- Policy forward ---\n",
    "            policy_in = state.to_policy_input()\n",
    "            policy_out = policy(policy_in)\n",
    "\n",
    "            dist = torch.distributions.Categorical(logits=policy_out.logits)\n",
    "            actions = dist.sample()\n",
    "            logprobs = dist.log_prob(actions)\n",
    "\n",
    "            # --- Store rollout step ---\n",
    "            buffer.add(RolloutStep(\n",
    "                obs=state.obs,\n",
    "                actions=actions,\n",
    "                rewards=state.rewards,\n",
    "                values=policy_out.values,\n",
    "                logprobs=logprobs,\n",
    "                terminated=state.terminated,\n",
    "                truncated=state.truncated,\n",
    "                hxs=policy_out.new_hxs,\n",
    "                cxs=policy_out.new_cxs,\n",
    "            ))\n",
    "\n",
    "            # --- Step environment ---\n",
    "            state = env.step(actions)\n",
    "\n",
    "            # --- Update hidden states in wrapper ---\n",
    "            env.update_hidden_states(policy_out.new_hxs, policy_out.new_cxs)\n",
    "\n",
    "\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # PPO update phase\n",
    "        # -----------------------------------------------------\n",
    "        for batch in buffer.recurrent_minibatches():\n",
    "\n",
    "            # --- Policy forward on batch ---\n",
    "            policy_in = VecPolicyInput(\n",
    "                obs=batch.obs,\n",
    "                hxs=batch.hxs,\n",
    "                cxs=batch.cxs,\n",
    "            )\n",
    "            out = policy(policy_in)\n",
    "\n",
    "            dist = torch.distributions.Categorical(logits=out.logits)\n",
    "            new_logprobs = dist.log_prob(batch.actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # --- PPO losses ---\n",
    "            ratio = (new_logprobs - batch.logprobs).exp()\n",
    "            surr1 = ratio * batch.advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * batch.advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = ((out.values - batch.returns) ** 2).mean()\n",
    "\n",
    "            # --- Add AR/TAR ---\n",
    "            ar_tar_loss = out.ar_loss + out.tar_loss\n",
    "\n",
    "            # --- Total loss ---\n",
    "            loss = (\n",
    "                policy_loss\n",
    "                + value_coef * value_loss\n",
    "                - entropy_coef * entropy\n",
    "                + ar_tar_loss\n",
    "            )\n",
    "\n",
    "            # --- Optimize ---\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Logging (optional)\n",
    "        # -----------------------------------------------------\n",
    "        print(f\"Update {update}: loss={loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4608714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_ppo_position_only_cartpole(cfg: PPOConfig):\n",
    "    device = torch.device(cfg.device)\n",
    "    env_id = \"POPGym-PositionOnlyCartPole-v0\"  # check id if needed\n",
    "\n",
    "    # Vectorized env\n",
    "    venv = SyncVectorEnv([make_env(env_id) for _ in range(cfg.num_envs)])\n",
    "    dummy_env = gym.make(env_id)\n",
    "    obs_shape = dummy_env.observation_space.shape\n",
    "    action_dim = dummy_env.action_space.n\n",
    "    dummy_env.close()\n",
    "\n",
    "    # Policy and optimizer\n",
    "    policy = LSTMPPOPolicy(\n",
    "        obs_dim=obs_shape[0],\n",
    "        action_dim=action_dim,\n",
    "        hidden_size=cfg.hidden_size\n",
    "    ).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "\n",
    "        # ==== PPO UPDATE ====\n",
    "        for epoch in range(cfg.update_epochs):\n",
    "            for mb in buffer.get_recurrent_minibatches(cfg.batch_envs):\n",
    "                mb_obs = mb[\"obs\"]          # (T,B,obs)\n",
    "                mb_actions = mb[\"actions\"]  # (T,B,1)\n",
    "                mb_returns = mb[\"returns\"]  # (T,B)\n",
    "                mb_advantages = mb[\"advantages\"]  # (T,B)\n",
    "                mb_logprobs_old = mb[\"logprobs\"]  # (T,B)\n",
    "                mb_hxs = mb[\"hxs\"]          # (B,H)\n",
    "                mb_cxs = mb[\"cxs\"]          # (B,H)\n",
    "\n",
    "                # Normalize advantages per minibatch\n",
    "                adv = mb_advantages\n",
    "                adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "                # Evaluate actions\n",
    "                new_logprobs, entropy, values = policy.evaluate_actions(\n",
    "                    mb_obs, mb_hxs, mb_cxs, mb_actions\n",
    "                )\n",
    "\n",
    "                # PPO ratio\n",
    "                ratio = (new_logprobs - mb_logprobs_old).exp()\n",
    "\n",
    "                # Policy loss\n",
    "                unclipped = ratio * adv\n",
    "                clipped = torch.clamp(ratio, 1.0 - cfg.clip_coef, 1.0 + cfg.clip_coef) * adv\n",
    "                policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values, mb_returns)\n",
    "\n",
    "                # Entropy bonus\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = policy_loss + cfg.vf_coef * value_loss - cfg.ent_coef * entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(policy.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "        # Logging (replace with your logger)\n",
    "        avg_return = mb_returns.mean().item()\n",
    "        print(f\"Step: {global_step}, ApproxReturn(last mb): {avg_return:.2f}\")\n",
    "\n",
    "    env.venv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07dd3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "import popgym\n",
    "from gymnasium.vector import SyncVectorEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b988d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class RecurrentVecEnvWrapper:\n",
    "    \"\"\"\n",
    "    Wraps a vectorized Gymnasium environment and manages\n",
    "    per-environment LSTM hidden states.\n",
    "\n",
    "    Works with SyncVectorEnv or AsyncVectorEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, venv, hidden_size, device):\n",
    "        self.venv = venv\n",
    "        self.num_envs = venv.num_envs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        # Hidden states per environment\n",
    "        self.hxs = torch.zeros(self.num_envs, hidden_size, device=device)\n",
    "        self.cxs = torch.zeros(self.num_envs, hidden_size, device=device)\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.venv.reset()\n",
    "\n",
    "        # Reset all hidden states\n",
    "        self.hxs.zero_()\n",
    "        self.cxs.zero_()\n",
    "\n",
    "        return torch.tensor(obs, device=self.device), info, self.hxs.clone(), self.cxs.clone()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        actions: (N, action_dim)\n",
    "        Returns:\n",
    "            obs: (N, obs_dim)\n",
    "            rewards: (N,)\n",
    "            terminated: (N,)\n",
    "            truncated: (N,)\n",
    "            info: list of dicts\n",
    "            hxs: (N, hidden_size)\n",
    "            cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        obs, rewards, terminated, truncated, info = self.venv.step(actions.cpu().numpy())\n",
    "\n",
    "        terminated = torch.tensor(terminated, device=self.device, dtype=torch.bool)\n",
    "        truncated = torch.tensor(truncated, device=self.device, dtype=torch.bool)\n",
    "\n",
    "        # Reset hidden states only for true terminals\n",
    "        done_mask = terminated  # NOT truncated\n",
    "        if done_mask.any():\n",
    "            self.hxs[done_mask] = 0\n",
    "            self.cxs[done_mask] = 0\n",
    "\n",
    "        return (\n",
    "            torch.tensor(obs, device=self.device),\n",
    "            torch.tensor(rewards, device=self.device),\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "            self.hxs.clone(),\n",
    "            self.cxs.clone(),\n",
    "        )\n",
    "\n",
    "    def update_hidden_states(self, new_hxs, new_cxs):\n",
    "        \"\"\"\n",
    "        Called after the policy forward pass.\n",
    "        new_hxs, new_cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        self.hxs.copy_(new_hxs)\n",
    "        self.cxs.copy_(new_cxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RecurrentRolloutBuffer:\n",
    "    def __init__(self, T, N, obs_shape, action_dim, hidden_size, device):\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.device = device\n",
    "\n",
    "        # Core storage\n",
    "        self.obs = torch.zeros(T, N, *obs_shape, device=device)\n",
    "        self.actions = torch.zeros(T, N, action_dim, device=device)\n",
    "        self.rewards = torch.zeros(T, N, device=device)\n",
    "        self.values = torch.zeros(T, N, device=device)\n",
    "        self.logprobs = torch.zeros(T, N, device=device)\n",
    "\n",
    "        # Episode termination logic\n",
    "        self.terminated = torch.zeros(T, N, device=device, dtype=torch.bool)\n",
    "        self.truncated = torch.zeros(T, N, device=device, dtype=torch.bool)\n",
    "\n",
    "        # Hidden states at the *start* of each timestep\n",
    "        # Shape: (T, N, num_layers, hidden_size)\n",
    "        self.hxs = torch.zeros(T, N, hidden_size, device=device)\n",
    "        self.cxs = torch.zeros(T, N, hidden_size, device=device)\n",
    "\n",
    "        # Filled index\n",
    "        self.step = 0\n",
    "\n",
    "    def add(self, obs, actions, rewards, values, logprobs,\n",
    "            terminated, truncated, hxs, cxs):\n",
    "        \"\"\"\n",
    "        obs: (N, obs_dim)\n",
    "        hxs, cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        t = self.step\n",
    "        self.obs[t].copy_(obs)\n",
    "        self.actions[t].copy_(actions)\n",
    "        self.rewards[t].copy_(rewards)\n",
    "        self.values[t].copy_(values)\n",
    "        self.logprobs[t].copy_(logprobs)\n",
    "        self.terminated[t].copy_(terminated)\n",
    "        self.truncated[t].copy_(truncated)\n",
    "        self.hxs[t].copy_(hxs)\n",
    "        self.cxs[t].copy_(cxs)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "    def compute_gae(self, last_value, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        last_value: (N,)\n",
    "        \"\"\"\n",
    "        T, N = self.T, self.N\n",
    "\n",
    "        advantages = torch.zeros(T, N, device=self.device)\n",
    "        last_gae = torch.zeros(N, device=self.device)\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            # True terminal: no bootstrap\n",
    "            true_terminal = self.terminated[t]\n",
    "\n",
    "            # Time-limit truncation: DO bootstrap\n",
    "            bootstrap = ~true_terminal\n",
    "\n",
    "            next_value = last_value if t == T - 1 else self.values[t + 1]\n",
    "\n",
    "            delta = (\n",
    "                self.rewards[t]\n",
    "                + gamma * next_value * bootstrap\n",
    "                - self.values[t]\n",
    "            )\n",
    "\n",
    "            last_gae = delta + gamma * lam * last_gae * bootstrap\n",
    "            advantages[t] = last_gae\n",
    "\n",
    "        returns = advantages + self.values\n",
    "        self.advantages = advantages\n",
    "        self.returns = returns\n",
    "\n",
    "    def get_recurrent_minibatches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns sequences of shape:\n",
    "        (seq_len=T, batch_size, ...)\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        env_indices = torch.randperm(N)\n",
    "\n",
    "        for start in range(0, N, batch_size):\n",
    "            idx = env_indices[start:start + batch_size]\n",
    "\n",
    "            yield {\n",
    "                \"obs\": self.obs[:, idx],\n",
    "                \"actions\": self.actions[:, idx],\n",
    "                \"values\": self.values[:, idx],\n",
    "                \"logprobs\": self.logprobs[:, idx],\n",
    "                \"returns\": self.returns[:, idx],\n",
    "                \"advantages\": self.advantages[:, idx],\n",
    "                \"hxs\": self.hxs[0, idx],  # initial hidden state\n",
    "                \"cxs\": self.cxs[0, idx],\n",
    "                \"terminated\": self.terminated[:, idx],\n",
    "                \"truncated\": self.truncated[:, idx],\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cage2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
