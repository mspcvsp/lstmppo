{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074171a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724cc159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mspcvsp/miniconda/envs/cage2_env/lib/python3.10/site-packages/gymnasium/envs/registration.py:520: UserWarning: \u001b[33mWARN: Using the latest versioned environment `popgym-PositionOnlyCartPoleEasy-v0` instead of the unversioned environment `popgym-PositionOnlyCartPoleEasy`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from lstmppo.init import initialize\n",
    "sys.argv = [\"\"]\n",
    "cfg = initialize(seconds_since_epoch=1766683383)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c162e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstmppo.env import make_env, RecurrentVecEnvWrapper\n",
    "from lstmppo.buffer import RecurrentRolloutBuffer\n",
    "from lstmppo.policy import LSTMPPOPolicy\n",
    "from gymnasium.vector import SyncVectorEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mspcvsp/miniconda/envs/cage2_env/lib/python3.10/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RecurrentVecEnvWrapper.__init__() got an unexpected keyword argument 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m RecurrentVecEnvWrapper(venv,\n\u001b[1;32m     12\u001b[0m                                           hidden_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m     13\u001b[0m                                           device\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m RecurrentRolloutBuffer(cfg)\n\u001b[0;32m---> 17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mPPOTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mPPOTrainer.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m LSTMPPOPolicy(cfg)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      8\u001b[0m venv \u001b[38;5;241m=\u001b[39m SyncVectorEnv([make_env(cfg\u001b[38;5;241m.\u001b[39menv_id)\n\u001b[1;32m      9\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mnum_envs)])\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[43mRecurrentVecEnvWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m RecurrentRolloutBuffer(cfg)\n",
      "\u001b[0;31mTypeError\u001b[0m: RecurrentVecEnvWrapper.__init__() got an unexpected keyword argument 'hidden_size'"
     ]
    }
   ],
   "source": [
    "class PPOTrainer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg):\n",
    "        \n",
    "        self.policy = LSTMPPOPolicy(cfg).to(cfg.device)\n",
    "\n",
    "        venv = SyncVectorEnv([make_env(cfg.env_id)\n",
    "                              for _ in range(cfg.num_envs)])\n",
    "        \n",
    "        self.env = RecurrentVecEnvWrapper(cfg,\n",
    "                                          venv)\n",
    "\n",
    "        self.buffer = RecurrentRolloutBuffer(cfg)\n",
    "\n",
    "trainer = PPOTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b497fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mspcvsp/miniconda/envs/cage2_env/lib/python3.10/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_ppo_position_only_cartpole(cfg: PPOConfig):\n",
    "    device = torch.device(cfg.device)\n",
    "    env_id = \"POPGym-PositionOnlyCartPole-v0\"  # check id if needed\n",
    "\n",
    "    # Vectorized env\n",
    "    venv = SyncVectorEnv([make_env(env_id) for _ in range(cfg.num_envs)])\n",
    "    dummy_env = gym.make(env_id)\n",
    "    obs_shape = dummy_env.observation_space.shape\n",
    "    action_dim = dummy_env.action_space.n\n",
    "    dummy_env.close()\n",
    "\n",
    "    # Policy and optimizer\n",
    "    policy = LSTMPPOPolicy(\n",
    "        obs_dim=obs_shape[0],\n",
    "        action_dim=action_dim,\n",
    "        hidden_size=cfg.hidden_size\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    obs, info, hxs, cxs = env.reset()\n",
    "\n",
    "    while global_step < cfg.total_steps:\n",
    "        buffer.reset()\n",
    "\n",
    "        # ==== COLLECT ROLLOUT ====\n",
    "        for t in range(cfg.rollout_steps):\n",
    "            with torch.no_grad():\n",
    "                actions, logprobs, values, new_hxs, new_cxs = policy.act(obs, hxs, cxs)\n",
    "\n",
    "            # Step env\n",
    "            next_obs, rewards, terminated, truncated, info, hxs_env, cxs_env = env.step(actions)\n",
    "\n",
    "            # Store step (use hxs_env/cxs_env = hidden state at start of step)\n",
    "            buffer.add(\n",
    "                obs=obs,\n",
    "                actions=actions,\n",
    "                rewards=rewards,\n",
    "                values=values,\n",
    "                logprobs=logprobs,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "                hxs=hxs_env,\n",
    "                cxs=cxs_env,\n",
    "            )\n",
    "\n",
    "            # Update hidden states inside env wrapper to new policy states\n",
    "            env.update_hidden_states(new_hxs, new_cxs)\n",
    "\n",
    "            obs = next_obs\n",
    "            hxs = new_hxs\n",
    "            cxs = new_cxs\n",
    "\n",
    "            global_step += cfg.num_envs\n",
    "\n",
    "        # Bootstrap value at last obs\n",
    "        with torch.no_grad():\n",
    "            last_logits, last_values, _, _ = policy.forward(obs, hxs, cxs)\n",
    "            last_value = last_values  # (N,)\n",
    "\n",
    "        buffer.compute_gae(last_value=last_value, gamma=cfg.gamma, lam=cfg.gae_lambda)\n",
    "\n",
    "        # ==== PPO UPDATE ====\n",
    "        for epoch in range(cfg.update_epochs):\n",
    "            for mb in buffer.get_recurrent_minibatches(cfg.batch_envs):\n",
    "                mb_obs = mb[\"obs\"]          # (T,B,obs)\n",
    "                mb_actions = mb[\"actions\"]  # (T,B,1)\n",
    "                mb_returns = mb[\"returns\"]  # (T,B)\n",
    "                mb_advantages = mb[\"advantages\"]  # (T,B)\n",
    "                mb_logprobs_old = mb[\"logprobs\"]  # (T,B)\n",
    "                mb_hxs = mb[\"hxs\"]          # (B,H)\n",
    "                mb_cxs = mb[\"cxs\"]          # (B,H)\n",
    "\n",
    "                # Normalize advantages per minibatch\n",
    "                adv = mb_advantages\n",
    "                adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "                # Evaluate actions\n",
    "                new_logprobs, entropy, values = policy.evaluate_actions(\n",
    "                    mb_obs, mb_hxs, mb_cxs, mb_actions\n",
    "                )\n",
    "\n",
    "                # PPO ratio\n",
    "                ratio = (new_logprobs - mb_logprobs_old).exp()\n",
    "\n",
    "                # Policy loss\n",
    "                unclipped = ratio * adv\n",
    "                clipped = torch.clamp(ratio, 1.0 - cfg.clip_coef, 1.0 + cfg.clip_coef) * adv\n",
    "                policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values, mb_returns)\n",
    "\n",
    "                # Entropy bonus\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = policy_loss + cfg.vf_coef * value_loss - cfg.ent_coef * entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(policy.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "        # Logging (replace with your logger)\n",
    "        avg_return = mb_returns.mean().item()\n",
    "        print(f\"Step: {global_step}, ApproxReturn(last mb): {avg_return:.2f}\")\n",
    "\n",
    "    env.venv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07dd3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "import popgym\n",
    "from gymnasium.vector import SyncVectorEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b988d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class RecurrentVecEnvWrapper:\n",
    "    \"\"\"\n",
    "    Wraps a vectorized Gymnasium environment and manages\n",
    "    per-environment LSTM hidden states.\n",
    "\n",
    "    Works with SyncVectorEnv or AsyncVectorEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, venv, hidden_size, device):\n",
    "        self.venv = venv\n",
    "        self.num_envs = venv.num_envs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        # Hidden states per environment\n",
    "        self.hxs = torch.zeros(self.num_envs, hidden_size, device=device)\n",
    "        self.cxs = torch.zeros(self.num_envs, hidden_size, device=device)\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.venv.reset()\n",
    "\n",
    "        # Reset all hidden states\n",
    "        self.hxs.zero_()\n",
    "        self.cxs.zero_()\n",
    "\n",
    "        return torch.tensor(obs, device=self.device), info, self.hxs.clone(), self.cxs.clone()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        actions: (N, action_dim)\n",
    "        Returns:\n",
    "            obs: (N, obs_dim)\n",
    "            rewards: (N,)\n",
    "            terminated: (N,)\n",
    "            truncated: (N,)\n",
    "            info: list of dicts\n",
    "            hxs: (N, hidden_size)\n",
    "            cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        obs, rewards, terminated, truncated, info = self.venv.step(actions.cpu().numpy())\n",
    "\n",
    "        terminated = torch.tensor(terminated, device=self.device, dtype=torch.bool)\n",
    "        truncated = torch.tensor(truncated, device=self.device, dtype=torch.bool)\n",
    "\n",
    "        # Reset hidden states only for true terminals\n",
    "        done_mask = terminated  # NOT truncated\n",
    "        if done_mask.any():\n",
    "            self.hxs[done_mask] = 0\n",
    "            self.cxs[done_mask] = 0\n",
    "\n",
    "        return (\n",
    "            torch.tensor(obs, device=self.device),\n",
    "            torch.tensor(rewards, device=self.device),\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "            self.hxs.clone(),\n",
    "            self.cxs.clone(),\n",
    "        )\n",
    "\n",
    "    def update_hidden_states(self, new_hxs, new_cxs):\n",
    "        \"\"\"\n",
    "        Called after the policy forward pass.\n",
    "        new_hxs, new_cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        self.hxs.copy_(new_hxs)\n",
    "        self.cxs.copy_(new_cxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RecurrentRolloutBuffer:\n",
    "    def __init__(self, T, N, obs_shape, action_dim, hidden_size, device):\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.device = device\n",
    "\n",
    "        # Core storage\n",
    "        self.obs = torch.zeros(T, N, *obs_shape, device=device)\n",
    "        self.actions = torch.zeros(T, N, action_dim, device=device)\n",
    "        self.rewards = torch.zeros(T, N, device=device)\n",
    "        self.values = torch.zeros(T, N, device=device)\n",
    "        self.logprobs = torch.zeros(T, N, device=device)\n",
    "\n",
    "        # Episode termination logic\n",
    "        self.terminated = torch.zeros(T, N, device=device, dtype=torch.bool)\n",
    "        self.truncated = torch.zeros(T, N, device=device, dtype=torch.bool)\n",
    "\n",
    "        # Hidden states at the *start* of each timestep\n",
    "        # Shape: (T, N, num_layers, hidden_size)\n",
    "        self.hxs = torch.zeros(T, N, hidden_size, device=device)\n",
    "        self.cxs = torch.zeros(T, N, hidden_size, device=device)\n",
    "\n",
    "        # Filled index\n",
    "        self.step = 0\n",
    "\n",
    "    def add(self, obs, actions, rewards, values, logprobs,\n",
    "            terminated, truncated, hxs, cxs):\n",
    "        \"\"\"\n",
    "        obs: (N, obs_dim)\n",
    "        hxs, cxs: (N, hidden_size)\n",
    "        \"\"\"\n",
    "        t = self.step\n",
    "        self.obs[t].copy_(obs)\n",
    "        self.actions[t].copy_(actions)\n",
    "        self.rewards[t].copy_(rewards)\n",
    "        self.values[t].copy_(values)\n",
    "        self.logprobs[t].copy_(logprobs)\n",
    "        self.terminated[t].copy_(terminated)\n",
    "        self.truncated[t].copy_(truncated)\n",
    "        self.hxs[t].copy_(hxs)\n",
    "        self.cxs[t].copy_(cxs)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "    def compute_gae(self, last_value, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        last_value: (N,)\n",
    "        \"\"\"\n",
    "        T, N = self.T, self.N\n",
    "\n",
    "        advantages = torch.zeros(T, N, device=self.device)\n",
    "        last_gae = torch.zeros(N, device=self.device)\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            # True terminal: no bootstrap\n",
    "            true_terminal = self.terminated[t]\n",
    "\n",
    "            # Time-limit truncation: DO bootstrap\n",
    "            bootstrap = ~true_terminal\n",
    "\n",
    "            next_value = last_value if t == T - 1 else self.values[t + 1]\n",
    "\n",
    "            delta = (\n",
    "                self.rewards[t]\n",
    "                + gamma * next_value * bootstrap\n",
    "                - self.values[t]\n",
    "            )\n",
    "\n",
    "            last_gae = delta + gamma * lam * last_gae * bootstrap\n",
    "            advantages[t] = last_gae\n",
    "\n",
    "        returns = advantages + self.values\n",
    "        self.advantages = advantages\n",
    "        self.returns = returns\n",
    "\n",
    "    def get_recurrent_minibatches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns sequences of shape:\n",
    "        (seq_len=T, batch_size, ...)\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        env_indices = torch.randperm(N)\n",
    "\n",
    "        for start in range(0, N, batch_size):\n",
    "            idx = env_indices[start:start + batch_size]\n",
    "\n",
    "            yield {\n",
    "                \"obs\": self.obs[:, idx],\n",
    "                \"actions\": self.actions[:, idx],\n",
    "                \"values\": self.values[:, idx],\n",
    "                \"logprobs\": self.logprobs[:, idx],\n",
    "                \"returns\": self.returns[:, idx],\n",
    "                \"advantages\": self.advantages[:, idx],\n",
    "                \"hxs\": self.hxs[0, idx],  # initial hidden state\n",
    "                \"cxs\": self.cxs[0, idx],\n",
    "                \"terminated\": self.terminated[:, idx],\n",
    "                \"truncated\": self.truncated[:, idx],\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cage2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
